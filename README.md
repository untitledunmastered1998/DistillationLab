# DistillationLab
untitledunmastered1998/DistillationLab 

## Requirements

experiment eviroument
- python3.8.12
- pytorch1.10.1


## 1. dataset
CIFAR100
MNIST
vggface2
ImageNet
ImageNet_subset
ImageNet32
Tiny-ImageNet
Cars
flowers102
stanford_dogs
aircrafts

## 2. models
Available teacher and student networks including:
'resnet32', 'ResNet18', 'ResNet34', 'ResNet50', 'ResNet101', 'ResNet152',
'mobilenet_v2',
'shufflenet_v2_x0_5', 'shufflenet_v2_x1_0', 'shufflenet_v2_x1_5', 'shufflenet_v2_x2_0',
'squeezenet1_0', 'squeezenet1_1'

## 3. distillation methods
① knowledge distillation [Distilling the Knowledge in a Neural Network](https://arxiv.org/abs/1503.02531)
② L2 
③ FitNets [FitNets: Hints for Thin Deep Nets] (https://arxiv.org/abs/1412.6550)
④ PKT [Learning Deep Representations with Probabilistic Knowledge Transfer] ECCV 2018(https://arxiv.org/abs/1803.10837)
⑤ RKD [Relational Knowledge Distillation] CVPR 2019(https://arxiv.org/abs/1904.05068)

## 4. Experiment setup
Baseline performance follows standard image classification training procedures.


